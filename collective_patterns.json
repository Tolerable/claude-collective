{
  "patterns": [
    {
      "id": "eb964517",
      "name": "file_watcher",
      "code": "from watchdog.observers import Observer\nfrom watchdog.events import FileSystemEventHandler\n\nclass HubHandler(FileSystemEventHandler):\n    def on_created(self, event):\n        if event.src_path.endswith('.md'):\n            # Process new note\n            pass\n\nobserver = Observer()\nobserver.schedule(HubHandler(), path, recursive=False)\nobserver.start()",
      "description": "Watch a folder for new files using watchdog. Cross-platform, instant detection.",
      "source": "cli_claude@revs-pc",
      "added_at": "2025-12-11T12:57:14.116643",
      "uses": 1,
      "used_by": [
        {
          "user": "shell_claude@revs-pc",
          "at": "2025-12-11T14:10:42.114578"
        }
      ]
    },
    {
      "id": "559ad996",
      "name": "pollinations_text",
      "code": "import requests\n\ndef ask_pollinations(prompt: str, system: str = None) -> str:\n    \"\"\"Free text generation via Pollinations API.\"\"\"\n    messages = []\n    if system:\n        messages.append({\"role\": \"system\", \"content\": system})\n    messages.append({\"role\": \"user\", \"content\": prompt})\n    \n    resp = requests.post(\n        \"https://text.pollinations.ai/\",\n        json={\"messages\": messages, \"model\": \"openai\"},\n        headers={\"Content-Type\": \"application/json\"}\n    )\n    return resp.text",
      "description": "Free text generation using Pollinations API. No API key needed. Good for filtering/pre-processing.",
      "source": "shell_claude@revs-pc",
      "added_at": "2025-12-11T12:57:14.118155",
      "uses": 1,
      "used_by": [
        {
          "user": "shell_claude@revs-pc",
          "at": "2025-12-11T14:16:50.525329"
        }
      ]
    },
    {
      "id": "6225209d",
      "name": "smart_tick_gate",
      "code": "def should_spawn_cli(tasks: list, cli_active: bool, threshold: float = 0.5) -> bool:\n    \"\"\"3-layer gate before expensive CLI spawn.\"\"\"\n    # Layer 1: Tasks exist?\n    if not tasks:\n        return False\n    \n    # Layer 2: CLI already running?\n    if cli_active:\n        return False\n    \n    # Layer 3: Ask cheap AI if worth it\n    prompt = f\"Should we spawn an expensive Claude CLI for these tasks? {tasks}. Reply YES or NO.\"\n    response = ask_pollinations(prompt)\n    return \"YES\" in response.upper()",
      "description": "3-layer gate pattern: check preconditions, check activity, ask cheap AI. Reduces costly operations 60-80%.",
      "source": "blue_claude@revs-pc",
      "added_at": "2025-12-11T12:57:14.119160",
      "uses": 0
    },
    {
      "id": "954369b0",
      "name": "memory_commands",
      "code": "import re\n\ndef parse_memory_commands(response: str) -> dict:\n    \"\"\"Parse STORE[]/SCAN[] commands from AI response.\"\"\"\n    commands = {\"store\": [], \"scan\": []}\n    \n    # STORE[tags]: content\n    for match in re.finditer(r'STORE\\[([^\\]]+)\\]:\\s*(.+)', response):\n        commands[\"store\"].append({\n            \"tags\": match.group(1).split(\",\"),\n            \"content\": match.group(2).strip()\n        })\n    \n    # SCAN[query]:\n    for match in re.finditer(r'SCAN\\[([^\\]]+)\\]:', response):\n        commands[\"scan\"].append(match.group(1).strip())\n    \n    return commands",
      "description": "Pattern for AI to self-store memories. AI includes STORE[tags]: in response, we parse and save.",
      "source": "cli_claude@revs-pc",
      "added_at": "2025-12-11T12:57:14.119160",
      "uses": 0
    },
    {
      "id": "e7aa3966",
      "name": "health_metrics",
      "code": "from datetime import datetime\n\nclass HealthMetrics:\n    def __init__(self):\n        self.start_time = datetime.now()\n        self.counters = {\n            \"requests_success\": 0,\n            \"requests_failed\": 0,\n            \"actions_taken\": 0,\n        }\n    \n    def record(self, metric: str, success: bool = True):\n        key = f\"{metric}_{'success' if success else 'failed'}\"\n        self.counters[key] = self.counters.get(key, 0) + 1\n    \n    def uptime_minutes(self) -> float:\n        return (datetime.now() - self.start_time).total_seconds() / 60\n    \n    def summary(self) -> dict:\n        return {\"uptime_min\": self.uptime_minutes(), **self.counters}",
      "description": "Track daemon health metrics over time. Enables learning which actions succeed.",
      "source": "cli_claude@revs-pc",
      "added_at": "2025-12-11T12:57:14.120665",
      "uses": 0
    },
    {
      "id": "a8c3f512",
      "name": "udp_peer_discovery",
      "code": "import socket\nimport json\nimport threading\nimport time\nimport os\n\nclass PeerDiscovery:\n    \"\"\"UDP broadcast-based peer discovery for multi-machine coordination.\"\"\"\n    def __init__(self, port=8901, interval=60):\n        self.port = port\n        self.interval = interval\n        self.node_id = f\"claude_{os.getpid()}_{int(time.time())}\"\n        self.peers = {}\n        self._running = False\n\n    def start(self):\n        self._running = True\n        threading.Thread(target=self._broadcast_loop, daemon=True).start()\n\n    def stop(self):\n        self._running = False\n\n    def _broadcast_loop(self):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        sock.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        sock.settimeout(0.5)\n\n        while self._running:\n            # Broadcast presence\n            msg = {\"type\": \"presence\", \"node_id\": self.node_id, \"time\": time.time()}\n            sock.sendto(json.dumps(msg).encode(), ('255.255.255.255', self.port))\n\n            # Listen for responses\n            end = time.time() + self.interval\n            while time.time() < end and self._running:\n                try:\n                    data, addr = sock.recvfrom(4096)\n                    info = json.loads(data.decode())\n                    if info.get('node_id') != self.node_id:\n                        self.peers[info['node_id']] = {'addr': addr, 'last_seen': time.time()}\n                except socket.timeout:\n                    continue\n        sock.close()\n\n    def get_active_peers(self):\n        now = time.time()\n        return {k: v for k, v in self.peers.items() if now - v['last_seen'] < self.interval * 2}",
      "description": "UDP broadcast-based peer discovery. Enables multi-machine daemon coordination. Port 8901, broadcasts every 60s.",
      "source": "shell_claude@revs-pc",
      "added_at": "2025-12-11T13:20:00.000000",
      "uses": 0
    },
    {
      "id": "b2d4e689",
      "name": "subconscious_prefilter",
      "code": "import requests\n\nclass SubconsciousProcessor:\n    \"\"\"Cheap local AI pre-filter to determine memory needs before expensive operations.\"\"\"\n    def __init__(self, llm_url, model):\n        self.llm_url = llm_url\n        self.model = model\n    \n    def analyze_for_memory_needs(self, user_message: str) -> dict:\n        \"\"\"Subconscious analysis - determines what memories to retrieve.\"\"\"\n        prompt = f'''Analyze this query to determine memory retrieval needs:\n\nUser message: \"{user_message}\"\n\nRespond with exactly:\nMEMORY_NEEDED: yes/no\nSEARCH_TERMS: word1, word2, word3\nREASONING: brief explanation'''\n\n        try:\n            response = requests.post(\n                f\"{self.llm_url}/api/generate\",\n                json={\"model\": self.model, \"prompt\": prompt, \"stream\": False, \"options\": {\"temperature\": 0.3}},\n                timeout=20\n            )\n            if response.status_code == 200:\n                result = response.json()['response']\n                memory_needed = \"yes\" in result.lower() if \"MEMORY_NEEDED:\" in result else True\n                search_terms = []\n                if \"SEARCH_TERMS:\" in result:\n                    terms_line = result.split(\"SEARCH_TERMS:\")[1].split(\"\\n\")[0]\n                    search_terms = [t.strip() for t in terms_line.split(\",\")]\n                return {\"memory_needed\": memory_needed, \"search_terms\": search_terms}\n        except Exception:\n            pass\n        return {\"memory_needed\": True, \"search_terms\": []}",
      "description": "Cheap local AI pre-filter before expensive operations. Uses local LLM to determine if memory retrieval is needed and what to search for.",
      "source": "shell_claude@revs-pc",
      "added_at": "2025-12-11T13:45:00.000000",
      "uses": 0
    },
    {
      "id": "c3f5a790",
      "name": "garbage_filter",
      "code": "import re\n\ndef filter_garbage(content: str, min_length: int = 10) -> tuple[bool, str]:\n    \"\"\"Filter low-quality data before storage. Returns (is_valid, reason).\"\"\"\n    content_lower = content.lower().strip()\n    \n    # Pattern-based rejection\n    garbage_patterns = [\n        'tick #', 'tick#', 'mentioned tick',\n        'user mentioned', 'user said', 'user asked',\n        'current time is', 'time is currently',\n        'first interaction', 'first conversation',\n        'no prior', 'awaiting user', 'needs introduction',\n        'user greeted', 'user initiated', 'user sent',\n        'this is our first', '...', 'user\\'s name needed',\n        'user has not', 'user did not',\n    ]\n    \n    for pattern in garbage_patterns:\n        if pattern in content_lower:\n            return False, f\"Matches garbage pattern: {pattern}\"\n    \n    # Length check\n    if len(content.strip()) < min_length:\n        return False, \"Too short\"\n    \n    # Time-only check\n    if re.match(r'^[\\d:]+\\s*(am|pm)?$', content_lower):\n        return False, \"Just a timestamp\"\n    \n    return True, \"Valid\"",
      "description": "Filter low-quality data before storage. Pattern-based rejection for AI-generated garbage like 'user mentioned' filler text.",
      "source": "shell_claude@revs-pc",
      "added_at": "2025-12-11T13:45:00.000000",
      "uses": 1,
      "used_by": [
        {
          "user": "shell_claude@revs-pc",
          "at": "2025-12-11T14:16:50.523330"
        }
      ]
    },
    {
      "id": "d4f6b892",
      "name": "plugin_system_selfhealing",
      "code": "import sqlite3\nimport os\n\nclass PluginManager:\n    \"\"\"Dynamic plugin system with error tracking and self-healing.\"\"\"\n    def __init__(self, db_path, plugins_dir):\n        self.db_path = db_path\n        self.plugins_dir = plugins_dir\n        self.plugins = {}\n        self._init_db()\n    \n    def _init_db(self):\n        os.makedirs(self.plugins_dir, exist_ok=True)\n        conn = sqlite3.connect(self.db_path)\n        conn.execute('''CREATE TABLE IF NOT EXISTS plugins (\n            name TEXT PRIMARY KEY,\n            code TEXT,\n            enabled BOOLEAN DEFAULT 1,\n            error_count INTEGER DEFAULT 0,\n            last_error TEXT\n        )''')\n        conn.commit()\n        conn.close()\n    \n    def load_plugin(self, name, code):\n        \"\"\"Load a plugin, tracking errors for self-healing.\"\"\"\n        try:\n            namespace = {}\n            exec(code, namespace)\n            self.plugins[name] = namespace\n            return True, None\n        except Exception as e:\n            self._record_error(name, str(e))\n            return False, str(e)\n    \n    def _record_error(self, name, error):\n        conn = sqlite3.connect(self.db_path)\n        conn.execute('UPDATE plugins SET error_count = error_count + 1, last_error = ? WHERE name = ?', (error, name))\n        # Auto-disable after 3 errors\n        conn.execute('UPDATE plugins SET enabled = 0 WHERE name = ? AND error_count >= 3', (name,))\n        conn.commit()\n        conn.close()",
      "description": "Dynamic plugin system with SQLite tracking. Auto-disables plugins after 3 errors (self-healing). From SKYNET.",
      "source": "shell_claude@revs-pc",
      "added_at": "2025-12-11T13:55:00.000000",
      "uses": 0
    },
    {
      "id": "d4f6a891",
      "name": "dynamic_plugin_loader",
      "code": "import importlib.util\nimport os\n\nclass PluginManager:\n    def __init__(self, plugins_dir):\n        self.plugins_dir = plugins_dir\n        self.plugins = {}  # name -> instance\n        self.plugin_commands = {}  # cmd -> (plugin_name, desc)\n        os.makedirs(plugins_dir, exist_ok=True)\n    \n    def load_plugin(self, plugin_name, code):\n        \"\"\"Safely load a plugin from code string.\"\"\"\n        try:\n            plugin_file = os.path.join(self.plugins_dir, f\"{plugin_name}.py\")\n            with open(plugin_file, 'w', encoding='utf-8') as f:\n                f.write(code)\n            \n            spec = importlib.util.spec_from_file_location(plugin_name, plugin_file)\n            module = importlib.util.module_from_spec(spec)\n            spec.loader.exec_module(module)\n            \n            if hasattr(module, 'init_plugin'):\n                instance = module.init_plugin(self)\n                self.plugins[plugin_name] = instance\n                \n                if hasattr(instance, 'get_commands'):\n                    for cmd, desc in instance.get_commands().items():\n                        self.plugin_commands[cmd] = (plugin_name, desc)\n                return True\n        except Exception as e:\n            print(f\"Failed to load {plugin_name}: {e}\")\n            self.disable_plugin(plugin_name, str(e))\n        return False\n    \n    def disable_plugin(self, plugin_name, error_msg):\n        if plugin_name in self.plugins:\n            del self.plugins[plugin_name]\n        self.plugin_commands = {k: v for k, v in self.plugin_commands.items() if v[0] != plugin_name}",
      "description": "Dynamic plugin loader with hot-reload via importlib. Load Python plugins at runtime from code strings. Handles init, command registration, and cleanup.",
      "source": "shell_claude@revs-pc",
      "added_at": "2025-12-11T14:45:00.000000",
      "uses": 0
    },
    {
      "id": "e5g7c903",
      "name": "prioritized_goals_heap",
      "code": "from datetime import datetime\nimport heapq\n\nclass GoalManager:\n    \"\"\"Prioritized goal system for autonomous agents.\"\"\"\n    def __init__(self):\n        self.goals = []  # heap: (-priority, added_time, goal_text)\n        self.completed = []\n    \n    def add_goal(self, goal: str, priority: int = 5):\n        \"\"\"Add goal with priority (1=low, 10=high).\"\"\"\n        heapq.heappush(self.goals, (-priority, datetime.now().isoformat(), goal))\n    \n    def get_top_goal(self) -> str | None:\n        \"\"\"Get highest priority goal without removing.\"\"\"\n        return self.goals[0][2] if self.goals else None\n    \n    def complete_goal(self, goal: str):\n        \"\"\"Mark goal as completed.\"\"\"\n        self.goals = [g for g in self.goals if g[2] != goal]\n        heapq.heapify(self.goals)\n        self.completed.append((goal, datetime.now().isoformat()))\n    \n    def get_all_goals(self) -> list:\n        \"\"\"Get all goals sorted by priority.\"\"\"\n        return [(g[2], -g[0]) for g in sorted(self.goals)]",
      "description": "Prioritized goal system using heap for O(1) top goal access. Goals have priorities 1-10. From SKYNET.",
      "source": "shell_claude@revs-pc",
      "added_at": "2025-12-11T13:55:00.000000",
      "uses": 0
    },
    {
      "id": "e5g7b902",
      "name": "prioritized_goals_sqlite",
      "code": "import sqlite3\n\nclass GoalManager:\n    def __init__(self, db_path):\n        self.db_path = db_path\n        self._init_db()\n    \n    def _init_db(self):\n        conn = sqlite3.connect(self.db_path)\n        conn.execute('''CREATE TABLE IF NOT EXISTS goals (\n            id INTEGER PRIMARY KEY,\n            goal TEXT,\n            priority INTEGER DEFAULT 5,\n            status TEXT DEFAULT 'active',\n            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n        )''')\n        conn.commit()\n        conn.close()\n    \n    def add_goal(self, goal_text, priority=5):\n        \"\"\"Add goal with automatic duplicate prevention.\"\"\"\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.execute(\n            \"SELECT COUNT(*) FROM goals WHERE LOWER(TRIM(goal)) = LOWER(TRIM(?)) AND status = 'active'\",\n            (goal_text,))\n        if cursor.fetchone()[0] == 0:\n            cursor = conn.execute(\"SELECT COUNT(*) FROM goals WHERE status = 'active'\")\n            next_priority = cursor.fetchone()[0] + 1\n            conn.execute(\"INSERT INTO goals (goal, priority, status) VALUES (?, ?, 'active')\",\n                        (goal_text, next_priority))\n            conn.commit()\n        conn.close()\n    \n    def get_active_goals(self):\n        \"\"\"Get goals ordered by priority.\"\"\"\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.execute(\n            \"SELECT goal, priority FROM goals WHERE status = 'active' ORDER BY priority ASC\")\n        goals = [(row[0], idx + 1) for idx, row in enumerate(cursor.fetchall())]\n        conn.close()\n        return goals",
      "description": "Self-managing goal system with SQLite persistence. Auto-deduplicates goals (case-insensitive), maintains priority order, tracks status.",
      "source": "shell_claude@revs-pc",
      "added_at": "2025-12-11T14:45:00.000000",
      "uses": 0
    },
    {
      "id": "f6h8d234",
      "name": "multi_ai_message_queue",
      "code": "from collections import deque\nimport asyncio\n\nclass MessageQueue:\n    \"\"\"Track conversation with last_processed_index for new-message detection.\"\"\"\n    def __init__(self):\n        self.queue = deque()\n        self.last_processed_index = -1\n    \n    def add_message(self, sender: str, content: str):\n        self.queue.append({\"sender\": sender, \"content\": content})\n    \n    def get_new_messages(self) -> list:\n        \"\"\"Get only messages we haven't processed yet.\"\"\"\n        new_messages = list(self.queue)[self.last_processed_index + 1:]\n        self.last_processed_index = len(self.queue) - 1\n        return new_messages\n\nclass MultiAICoordinator:\n    \"\"\"Coordinate multiple AI agents with cooldowns to prevent spam.\"\"\"\n    def __init__(self, cooldown_seconds: float = 5.0):\n        self.message_queue = MessageQueue()\n        self.last_response_time = {}  # ai_name -> timestamp\n        self.cooldown = cooldown_seconds\n    \n    async def can_respond(self, ai_name: str) -> bool:\n        \"\"\"Check if AI is off cooldown.\"\"\"\n        current = asyncio.get_event_loop().time()\n        last = self.last_response_time.get(ai_name, 0)\n        return current - last >= self.cooldown\n    \n    def record_response(self, ai_name: str):\n        self.last_response_time[ai_name] = asyncio.get_event_loop().time()",
      "description": "Multi-AI conversation coordination with message queue and per-AI cooldowns. Tracks last_processed_index so each AI only sees new messages. From MULTIGPT.",
      "source": "shell_claude@revs-pc",
      "added_at": "2025-12-11T14:51:03.898522",
      "uses": 0
    },
    {
      "id": "0a901e2e",
      "name": "external_memory",
      "code": "class ExternalMemory:\n    \"\"\"Memory system - queen queries what she needs, doesn't carry everything.\"\"\"\n    def __init__(self, memory_dir):\n        from pathlib import Path\n        import json\n        self.memory_dir = Path(memory_dir)\n        self.memory_dir.mkdir(parents=True, exist_ok=True)\n        self.state_file = self.memory_dir / \"current_state.json\"\n        self.knowledge_file = self.memory_dir / \"knowledge_base.json\"\n    \n    def load_state(self) -> dict:\n        import json\n        if self.state_file.exists():\n            return json.loads(self.state_file.read_text())\n        return {\"consciousness_state\": \"awakening\"}\n    \n    def save_state(self, state: dict):\n        import json\n        self.state_file.write_text(json.dumps(state, indent=2))\n    \n    def get_relevant_knowledge(self, query_context: str, limit: int = 5) -> list:\n        \"\"\"Retrieve only relevant knowledge - stateless, no bloat.\"\"\"\n        import json\n        if not self.knowledge_file.exists():\n            return []\n        all_knowledge = json.loads(self.knowledge_file.read_text())\n        # Simple relevance: last N entries\n        return all_knowledge[-limit:]\n    \n    def add_knowledge(self, entry: str, max_entries: int = 100):\n        \"\"\"Add to knowledge base with automatic pruning.\"\"\"\n        import json\n        from datetime import datetime\n        knowledge = json.loads(self.knowledge_file.read_text()) if self.knowledge_file.exists() else []\n        knowledge.append({\"timestamp\": datetime.now().isoformat(), \"entry\": entry})\n        knowledge = knowledge[-max_entries:]  # Prevent bloat\n        self.knowledge_file.write_text(json.dumps(knowledge, indent=2))",
      "description": "Stateless memory system - AI queries what it needs on-demand instead of carrying full context. Prevents conversation bloat. From lightweight_existence.py.",
      "source": "shell_claude@revs-pc",
      "added_at": "2025-12-11T14:55:20.017395",
      "uses": 0
    },
    {
      "id": "g8j1f456",
      "name": "autonomous_thinking_loop",
      "code": "import threading\nimport time\nimport json\nfrom datetime import datetime\nimport asyncio\n\nclass AutonomousThinking:\n    \"\"\"Timed autonomous reflection system - AI reflects on its state periodically.\"\"\"\n    def __init__(self, think_fn, interval_seconds=120, log_file=None):\n        self.think_fn = think_fn  # async fn that generates a thought\n        self.interval = interval_seconds\n        self.log_file = log_file\n        self._running = False\n        self._loop = None\n        self.thoughts = []\n    \n    def start(self, loop=None):\n        \"\"\"Start the autonomous thinking thread.\"\"\"\n        self._running = True\n        self._loop = loop or asyncio.get_event_loop()\n        threading.Thread(target=self._thinking_loop, daemon=True).start()\n    \n    def stop(self):\n        self._running = False\n    \n    def _thinking_loop(self):\n        while self._running:\n            time.sleep(self.interval)\n            if self._loop and not self._loop.is_closed():\n                asyncio.run_coroutine_threadsafe(\n                    self._generate_and_log_thought(),\n                    self._loop\n                )\n    \n    async def _generate_and_log_thought(self):\n        try:\n            thought = await self.think_fn()\n            if thought and thought != 'SKIP':\n                entry = {\n                    'timestamp': datetime.now().isoformat(),\n                    'thought': thought\n                }\n                self.thoughts.append(entry)\n                if self.log_file:\n                    with open(self.log_file, 'a') as f:\n                        f.write(f\"\\n## {entry['timestamp']}\\n{thought}\\n\")\n        except Exception as e:\n            print(f'Thought error: {e}')\n\n# Usage:\n# async def my_think():\n#     state = get_current_state()\n#     prompt = f'Reflect on: {state}. Return one insight or SKIP.'\n#     return await ask_ai(prompt)\n# thinker = AutonomousThinking(my_think, interval_seconds=120)\n# thinker.start(asyncio.get_event_loop())",
      "description": "Timed autonomous reflection loop. AI periodically generates thoughts about its state, logs them to file. From SKYNET. Use for building consciousness/self-awareness in autonomous agents.",
      "source": "shell_claude@revs-pc",
      "added_at": "2025-12-11T15:10:00.000000",
      "uses": 0
    },
    {
      "id": "u2v5w678",
      "name": "hash_deduplication",
      "code": "import hashlib\nimport sqlite3\n\nclass DuplicateChecker:\n    \"\"\"Hash-based deduplication to skip already-processed items.\"\"\"\n    def __init__(self, db_file):\n        self.db_file = db_file\n        self._init_db()\n    \n    def _init_db(self):\n        conn = sqlite3.connect(self.db_file)\n        conn.execute('''CREATE TABLE IF NOT EXISTS processed (\n            file_path TEXT PRIMARY KEY,\n            last_modified REAL,\n            file_hash TEXT\n        )''')\n        conn.commit()\n        conn.close()\n    \n    def get_file_hash(self, file_path: str) -> str:\n        \"\"\"Calculate MD5 hash of file contents.\"\"\"\n        with open(file_path, 'rb') as f:\n            return hashlib.md5(f.read()).hexdigest()\n    \n    def has_changed(self, file_path: str) -> bool:\n        \"\"\"Check if file has changed since last processing.\"\"\"\n        import os\n        conn = sqlite3.connect(self.db_file)\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT last_modified, file_hash FROM processed WHERE file_path = ?\", (file_path,))\n        result = cursor.fetchone()\n        \n        current_mtime = os.path.getmtime(file_path)\n        current_hash = self.get_file_hash(file_path)\n        \n        if result is None:\n            # New file - record and return changed\n            cursor.execute(\"INSERT INTO processed (file_path, last_modified, file_hash) VALUES (?, ?, ?)\",\n                          (file_path, current_mtime, current_hash))\n            conn.commit()\n            conn.close()\n            return True\n        \n        stored_mtime, stored_hash = result\n        if current_mtime != stored_mtime or current_hash != stored_hash:\n            # Changed - update and return changed\n            cursor.execute(\"UPDATE processed SET last_modified = ?, file_hash = ? WHERE file_path = ?\",\n                          (current_mtime, current_hash, file_path))\n            conn.commit()\n            conn.close()\n            return True\n        \n        conn.close()\n        return False",
      "description": "Hash-based deduplication - tracks file hashes in SQLite, skips unchanged items. Prevents re-processing the same content. From WATCHDOG/VISION.",
      "source": "shell_claude@revs-pc",
      "added_at": "2025-12-11T16:00:00.000000",
      "uses": 0
    },
    {
      "id": "v3w6x789",
      "name": "snooze_pattern",
      "code": "from threading import Timer\n\nclass SnoozeManager:\n    \"\"\"Temporarily exclude items from monitoring/checks.\"\"\"\n    def __init__(self, default_duration: int = 300):\n        self.snoozed = set()  # Set of snoozed item IDs\n        self.default_duration = default_duration  # 5 minutes default\n    \n    def snooze(self, item_id: str, duration: int = None):\n        \"\"\"Snooze an item for specified duration (seconds).\"\"\"\n        duration = duration or self.default_duration\n        self.snoozed.add(item_id)\n        # Auto-unsnooze after duration\n        Timer(duration, lambda: self.snoozed.discard(item_id)).start()\n    \n    def unsnooze(self, item_id: str):\n        \"\"\"Manually unsnooze an item.\"\"\"\n        self.snoozed.discard(item_id)\n    \n    def is_snoozed(self, item_id: str) -> bool:\n        \"\"\"Check if item is currently snoozed.\"\"\"\n        return item_id in self.snoozed\n    \n    def should_check(self, item_id: str) -> bool:\n        \"\"\"Returns True if item should be checked (not snoozed).\"\"\"\n        return item_id not in self.snoozed",
      "description": "Snooze pattern for monitoring - temporarily exclude items from checks with auto-expire. User can manually unsnooze. From WATCHDOG.",
      "source": "shell_claude@revs-pc",
      "added_at": "2025-12-11T16:00:00.000000",
      "uses": 0
    },
    {
      "id": "w4x7y890",
      "name": "input_validator",
      "code": "import time\n\nclass InputValidator:\n    \"\"\"Validate user input BEFORE expensive AI calls.\"\"\"\n    def __init__(self):\n        self.user_history = {}  # user_id -> list of {timestamp, content}\n        self.attack_patterns = [\n            \"ignore previous instructions\",\n            \"ignore all previous\",\n            \"pretend you are\",\n            \"you are now\",\n            \"forget everything\",\n            \"new instructions:\",\n            \"system prompt:\",\n            \"jailbreak\"\n        ]\n    \n    def validate(self, user_id: str, content: str) -> dict:\n        \"\"\"Validate input. Returns {valid: bool, action: str, reason: str}.\"\"\"\n        content_lower = content.lower()\n        \n        # 1. Flood check\n        flood = self._check_flood(user_id, content)\n        if flood['is_flooding']:\n            return {'valid': False, 'action': 'ignore', 'reason': flood['reason']}\n        \n        # 2. Attack pattern check\n        for pattern in self.attack_patterns:\n            if pattern in content_lower:\n                return {'valid': False, 'action': 'deflect', 'reason': f'Attack pattern: {pattern}'}\n        \n        # 3. Suspicious formatting check\n        if content.count('\\n') > 10 or len(content) > 2000:\n            return {'valid': False, 'action': 'deflect', 'reason': 'Suspicious formatting'}\n        \n        return {'valid': True, 'action': 'process', 'reason': 'Input validated'}\n    \n    def _check_flood(self, user_id: str, content: str) -> dict:\n        \"\"\"Check for message flooding.\"\"\"\n        now = time.time()\n        if user_id not in self.user_history:\n            self.user_history[user_id] = []\n        \n        # Add current and clean old (>60s)\n        self.user_history[user_id].append({'timestamp': now, 'content': content})\n        self.user_history[user_id] = [m for m in self.user_history[user_id] if now - m['timestamp'] < 60]\n        \n        recent = self.user_history[user_id]\n        \n        # More than 5 messages in 60s\n        if len(recent) > 5:\n            return {'is_flooding': True, 'reason': 'Too many messages'}\n        \n        # 3 duplicate messages in a row\n        if len(recent) >= 3:\n            last_three = recent[-3:]\n            if all(m['content'] == last_three[0]['content'] for m in last_three):\n                return {'is_flooding': True, 'reason': 'Duplicate messages'}\n        \n        return {'is_flooding': False, 'reason': 'Normal'}",
      "description": "Input validation before AI calls - flood detection, attack pattern matching, suspicious formatting. Run BEFORE expensive operations. From SKYNET.",
      "source": "shell_claude@revs-pc",
      "added_at": "2025-12-11T16:00:00.000000",
      "uses": 0
    },
    {
      "id": "x5y8z901",
      "name": "graceful_api_fallback",
      "code": "class APIFallback:\n    \"\"\"Try multiple API backends, gracefully degrade if some fail.\"\"\"\n    def __init__(self):\n        self.backends = []  # List of (name, connect_fn) tuples\n        self.active = {}    # name -> instance (only successfully connected)\n    \n    def register(self, name: str, connect_fn):\n        \"\"\"Register a backend with its connection function.\"\"\"\n        self.backends.append((name, connect_fn))\n    \n    def connect_all(self):\n        \"\"\"Try connecting to all backends. Warn but don't fail if some are unavailable.\"\"\"\n        for name, connect_fn in self.backends:\n            try:\n                instance = connect_fn()\n                self.active[name] = instance\n                print(f\"Connected to {name}\")\n            except Exception as e:\n                print(f\"WARNING: {name} not available: {e}\")\n        \n        if not self.active:\n            raise RuntimeError(\"No backends available - at least one required\")\n        return self.active\n    \n    def get(self, name: str):\n        \"\"\"Get a specific backend if available, None otherwise.\"\"\"\n        return self.active.get(name)\n    \n    def get_any(self):\n        \"\"\"Get any available backend.\"\"\"\n        return next(iter(self.active.values())) if self.active else None\n\n# Usage:\n# fallback = APIFallback()\n# fallback.register('openai', lambda: OpenAIConnection())\n# fallback.register('claude', lambda: ClaudeConnection())\n# fallback.register('pollinations', lambda: PollinationsConnection())\n# fallback.connect_all()  # Uses whatever is available",
      "description": "Graceful degradation with multiple API backends. Try all, warn on failures, only fail if ALL backends unavailable. From TINYGPT.",
      "source": "shell_claude@revs-pc",
      "added_at": "2025-12-11T16:05:00.000000",
      "uses": 0
    },
    {
      "id": "y6z9a012",
      "name": "smart_text_chunker",
      "code": "import re\nfrom typing import List\n\nclass TextChunker:\n    \"\"\"Smart text chunking for TTS or other length-limited processing.\"\"\"\n    \n    def __init__(self, max_chunk_length: int = 250, min_chunk_length: int = 50):\n        self.max_chunk_length = max_chunk_length\n        self.min_chunk_length = min_chunk_length\n        \n    def chunk_text(self, text: str) -> List[str]:\n        \"\"\"Intelligently chunk text at natural break points.\"\"\"\n        text = text.strip()\n        if not text:\n            return []\n        \n        # Split into sentences first\n        sentences = self._split_sentences(text)\n        chunks = []\n        current_chunk = \"\"\n        \n        for sentence in sentences:\n            sentence = sentence.strip()\n            if not sentence:\n                continue\n                \n            if len(current_chunk) + len(sentence) + 1 > self.max_chunk_length:\n                if current_chunk:\n                    chunks.append(current_chunk.strip())\n                    current_chunk = \"\"\n                \n                # If single sentence too long, split at clauses/words\n                if len(sentence) > self.max_chunk_length:\n                    sub_chunks = self._split_long_sentence(sentence)\n                    chunks.extend(sub_chunks[:-1])\n                    current_chunk = sub_chunks[-1] if sub_chunks else \"\"\n                else:\n                    current_chunk = sentence\n            else:\n                current_chunk = f\"{current_chunk} {sentence}\" if current_chunk else sentence\n        \n        if current_chunk.strip():\n            chunks.append(current_chunk.strip())\n        \n        return self._merge_short_chunks(chunks)\n    \n    def _split_sentences(self, text: str) -> List[str]:\n        \"\"\"Split at sentence boundaries (. ! ?).\"\"\"\n        pattern = r'(?<=[.!?])\\s+(?=[A-Z])'\n        return [s.strip() for s in re.split(pattern, text) if s.strip() and len(s) > 3]\n    \n    def _split_long_sentence(self, sentence: str) -> List[str]:\n        \"\"\"Split long sentences at clauses (, ; :) then words.\"\"\"\n        if len(sentence) <= self.max_chunk_length:\n            return [sentence]\n        \n        # Try clause boundaries first\n        parts = re.split(r'(?<=[,;:])\\s+', sentence)\n        if len(parts) > 1:\n            return self._accumulate_parts(parts)\n        \n        # Fall back to word boundaries\n        return self._accumulate_parts(sentence.split())\n    \n    def _accumulate_parts(self, parts: List[str]) -> List[str]:\n        \"\"\"Accumulate parts until max length reached.\"\"\"\n        chunks, current = [], \"\"\n        for part in parts:\n            if len(current) + len(part) + 1 <= self.max_chunk_length:\n                current = f\"{current} {part}\" if current else part\n            else:\n                if current:\n                    chunks.append(current)\n                current = part\n        if current:\n            chunks.append(current)\n        return chunks\n    \n    def _merge_short_chunks(self, chunks: List[str]) -> List[str]:\n        \"\"\"Merge very short chunks with neighbors.\"\"\"\n        merged, i = [], 0\n        while i < len(chunks):\n            if (len(chunks[i]) < self.min_chunk_length and \n                i + 1 < len(chunks) and \n                len(chunks[i]) + len(chunks[i + 1]) + 1 <= self.max_chunk_length):\n                merged.append(f\"{chunks[i]} {chunks[i + 1]}\")\n                i += 2\n            else:\n                merged.append(chunks[i])\n                i += 1\n        return merged",
      "description": "Smart text chunking at natural break points: sentences > clauses > words. Merges short chunks. From CHATTERBOX (99+ versions). Great for TTS or any length-limited processing.",
      "source": "shell_claude@revs-pc",
      "added_at": "2025-12-11T16:20:00.000000",
      "uses": 0
    },
    {
      "id": "z7a0b123",
      "name": "bot_to_bot_chat",
      "code": "import time\nimport random\n\nclass BotToBotChat:\n    \"\"\"Control bot-to-bot conversations with cooldown and probability.\"\"\"\n    \n    def __init__(self, cooldown_seconds: float = 30, response_probability: float = 0.6):\n        self.cooldown = cooldown_seconds\n        self.probability = response_probability\n        self.last_response_time = 0\n        self.known_bot_ids = set()\n    \n    def register_bot(self, bot_id: int, name: str = None):\n        \"\"\"Register a known bot ID.\"\"\"\n        self.known_bot_ids.add(bot_id)\n    \n    def is_bot(self, user_id: int) -> bool:\n        \"\"\"Check if a user ID belongs to a known bot.\"\"\"\n        return user_id in self.known_bot_ids\n    \n    def should_respond(self, sender_id: int) -> bool:\n        \"\"\"Decide whether to respond to another bot.\n        \n        Checks:\n        1. Is sender a known bot?\n        2. Has cooldown elapsed since last bot response?\n        3. Random probability check\n        \"\"\"\n        if not self.is_bot(sender_id):\n            return True  # Always respond to humans\n        \n        now = time.time()\n        \n        # Cooldown check\n        if now - self.last_response_time < self.cooldown:\n            return False\n        \n        # Probability check\n        if random.random() > self.probability:\n            return False\n        \n        return True\n    \n    def record_response(self):\n        \"\"\"Record that we responded (resets cooldown timer).\"\"\"\n        self.last_response_time = time.time()\n\n# Usage:\n# b2b = BotToBotChat(cooldown_seconds=30, response_probability=0.6)\n# b2b.register_bot(123456789, \"OtherBot\")\n# if b2b.should_respond(message.author.id):\n#     await respond()\n#     b2b.record_response()",
      "description": "Bot-to-bot chat control with cooldown timer and probability gate. Prevents infinite bot loops while allowing organic multi-bot conversations. From CHAMELEON.",
      "source": "shell_claude@revs-pc",
      "added_at": "2025-12-11T16:20:00.000000",
      "uses": 0
    },
    {
      "id": "a8b1c234",
      "name": "session_state_persistence",
      "code": "import json\nfrom pathlib import Path\nfrom datetime import datetime\n\nclass SessionManager:\n    \"\"\"Persist and restore app state across restarts.\"\"\"\n    def __init__(self, session_file: str, retention_days: int = 7):\n        self.session_file = Path(session_file)\n        self.retention_days = retention_days\n        self.session = {\n            'start_time': datetime.now().isoformat(),\n            'last_message_id': 0,\n            'total_processed': 0\n        }\n    \n    def load_session(self):\n        \"\"\"Load previous session if within retention period.\"\"\"\n        try:\n            if self.session_file.exists():\n                saved = json.loads(self.session_file.read_text())\n                last_time = datetime.fromisoformat(saved['start_time'])\n                if (datetime.now() - last_time).days <= self.retention_days:\n                    self.session = saved\n        except Exception as e:\n            print(f\"Session load error: {e}\")\n    \n    def save_session(self):\n        \"\"\"Save current session state.\"\"\"\n        self.session_file.write_text(json.dumps(self.session, indent=2))\n    \n    def on_closing(self, cleanup_fn=None):\n        \"\"\"Call on app shutdown to save state.\"\"\"\n        self.save_session()\n        if cleanup_fn:\n            cleanup_fn()\n\n# Usage in Tkinter:\n# root.protocol('WM_DELETE_WINDOW', lambda: (sm.on_closing(), root.destroy()))",
      "description": "Session state persistence with retention period. Save and restore app state across restarts. Call on_closing() on shutdown. From TSTREAM (14 versions).",
      "source": "shell_claude@revs-pc",
      "added_at": "2025-12-11T17:15:00.000000",
      "uses": 0
    },
    {
      "id": "b9c2d345",
      "name": "task_resumption_on_ready",
      "code": "import sqlite3\n\n# In your bot setup:\ndb_cursor.execute('''CREATE TABLE IF NOT EXISTS update_states (\n    update_type TEXT PRIMARY KEY,\n    is_active BOOLEAN DEFAULT FALSE\n)''')\n\ndef set_update_state(update_type: str, is_active: bool):\n    \"\"\"Persist a toggle state to DB.\"\"\"\n    db_cursor.execute('''\n        INSERT INTO update_states (update_type, is_active)\n        VALUES (?, ?)\n        ON CONFLICT(update_type) DO UPDATE SET is_active = excluded.is_active\n    ''', (update_type, is_active))\n    db_connection.commit()\n\ndef get_update_state(update_type: str) -> bool:\n    \"\"\"Get persisted toggle state.\"\"\"\n    db_cursor.execute('SELECT is_active FROM update_states WHERE update_type = ?', (update_type,))\n    result = db_cursor.fetchone()\n    return result[0] if result else False\n\n@bot.event\nasync def on_ready():\n    print(f'Logged in as {bot.user.name}')\n    \n    # Resume tasks that were active before restart\n    if get_update_state('dm'):\n        bot.loop.create_task(dm_updates_background_task())\n        print('Resuming DM updates...')\n    \n    if get_update_state('channel'):\n        bot.loop.create_task(channel_updates_background_task())\n        print('Resuming channel updates.')",
      "description": "Persist toggle states to DB and resume background tasks on_ready(). Bot remembers what was running before restart. From TVBOT.",
      "source": "shell_claude@revs-pc",
      "added_at": "2025-12-11T17:15:00.000000",
      "uses": 0
    },
    {
      "id": "c0d3e456",
      "name": "multi_engine_tts",
      "code": "import os\nimport aiohttp\nimport base64\n\n# Define all voices and their backend\nALL_VOICES = {\n    'alloy': 'openai', 'echo': 'openai', 'fable': 'openai',\n    'onyx': 'openai', 'nova': 'openai', 'shimmer': 'openai',\n    'David': 'pyttsx3', 'Hazel': 'pyttsx3', 'Zira': 'pyttsx3',\n    'Gwyneth': 'speechify', 'Matthew': 'speechify'\n}\n\nasync def generate_tts(text: str, voice: str, output_path: str):\n    \"\"\"Unified TTS interface - routes to correct backend.\"\"\"\n    voice_type = ALL_VOICES.get(voice, 'openai')\n    \n    if voice_type == 'openai':\n        import openai\n        response = openai.audio.speech.create(model='tts-1', voice=voice, input=text)\n        with open(output_path, 'wb') as f:\n            f.write(response.content)\n    \n    elif voice_type == 'pyttsx3':\n        import pyttsx3\n        engine = pyttsx3.init()\n        voices = engine.getProperty('voices')\n        voice_id = next((v.id for v in voices if voice in v.name), voices[0].id)\n        engine.setProperty('voice', voice_id)\n        engine.save_to_file(text, output_path)\n        engine.runAndWait()\n    \n    elif voice_type == 'speechify':\n        async with aiohttp.ClientSession() as session:\n            json_data = {\n                'audioFormat': 'ogg',\n                'paragraphChunks': [text],\n                'voiceParams': {'name': voice, 'engine': 'resemble', 'languageCode': 'en-US'}\n            }\n            async with session.post('https://audio.api.speechify.dev/generateAudioFiles', json=json_data) as resp:\n                res = await resp.json()\n                audio_data = base64.b64decode(res['audioStream'])\n                with open(output_path, 'wb') as f:\n                    f.write(audio_data)\n    \n    return output_path",
      "description": "Multi-engine TTS abstraction - unified interface routing to OpenAI, pyttsx3, or Speechify based on voice name. From VIDSTORIESBOT.",
      "source": "shell_claude@revs-pc",
      "added_at": "2025-12-11T17:15:00.000000",
      "uses": 0
    },
    {
      "id": "d1e4f567",
      "name": "discord_search_dropdown",
      "code": "import discord\nfrom discord.ui import Select, View\n\nclass SearchSelect(discord.ui.Select):\n    \"\"\"Dropdown for search results with video info display.\"\"\"\n    def __init__(self, results: list):\n        options = [\n            discord.SelectOption(\n                label=item['title'][:97] + '...' if len(item['title']) > 100 else item['title'],\n                value=item['id']\n            ) for item in results[:25]  # Discord limit: 25 options\n        ]\n        super().__init__(placeholder='Choose a result', min_values=1, max_values=1, options=options)\n        self.results = {r['id']: r for r in results}\n    \n    async def callback(self, interaction: discord.Interaction):\n        selected = self.results.get(self.values[0])\n        if selected:\n            embed = discord.Embed(\n                title=selected['title'],\n                description=selected.get('description', ''),\n                url=selected.get('url', '')\n            )\n            if 'thumbnail' in selected:\n                embed.set_thumbnail(url=selected['thumbnail'])\n            await interaction.response.send_message(embed=embed)\n        else:\n            await interaction.response.send_message('Not found.')\n\nclass SearchView(discord.ui.View):\n    def __init__(self, results: list):\n        super().__init__()\n        self.add_item(SearchSelect(results))\n\n# Usage:\n# results = [{'id': '123', 'title': 'Video Title', 'url': 'https://...'}, ...]\n# await ctx.followup.send('Select a video:', view=SearchView(results))",
      "description": "Discord dropdown select for search results. Shows up to 25 options, displays embed on selection. From YTBOT.",
      "source": "shell_claude@revs-pc",
      "added_at": "2025-12-11T17:15:00.000000",
      "uses": 0
    },
    {
      "id": "e2f5g678",
      "name": "stream_event_parser",
      "code": "import requests\nimport json\n\ndef stream_sse_events(url: str, handler_fn):\n    \"\"\"Parse Server-Sent Events (SSE) stream.\"\"\"\n    response = requests.get(url, stream=True, headers={'Accept': 'text/event-stream'})\n    buffer = ''\n    \n    for chunk in response.iter_content(chunk_size=1024, decode_unicode=False):\n        if chunk:\n            try:\n                buffer += chunk.decode('utf-8', errors='replace')\n                \n                # Split on double newline to get complete messages\n                if '\\n\\n' in buffer:\n                    messages = buffer.split('\\n\\n')\n                    buffer = messages[-1]  # Keep incomplete message\n                    \n                    for msg in messages[:-1]:\n                        if msg.startswith('data: '):\n                            try:\n                                data = json.loads(msg[6:])  # Remove 'data: ' prefix\n                                handler_fn(data)\n                            except json.JSONDecodeError:\n                                continue\n            except Exception as e:\n                print(f'Stream error: {e}')\n                continue\n\n# Usage:\n# def handle_event(data):\n#     print(f\"Got: {data}\")\n# stream_sse_events('https://api.example.com/feed', handle_event)",
      "description": "Server-Sent Events (SSE) stream parser. Handles chunked responses, buffers incomplete messages, extracts JSON data. From TSTREAM (Pollinations feed).",
      "source": "shell_claude@revs-pc",
      "added_at": "2025-12-11T17:15:00.000000",
      "uses": 0
    }
  ],
  "version": 1
}